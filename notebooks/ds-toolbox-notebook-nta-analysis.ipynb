{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non Target Analysis of Environmental Mass Spectrometry Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{bdg-link-info}`Reference [1] <https://link.springer.com/article/10.1007/s00216-015-8681-7>`\n",
    "{bdg-link-info}`Reference [2] <https://doi.org/10.1016/j.trac.2023.116944>`\n",
    "{bdg-link-info}`Reference [3] <https://doi.org/10.1016/j.trac.2019.04.008>`\n",
    "{bdg-link-info}`Reference [4] <https://pubmed.ncbi.nlm.nih.gov/36174727/>`\n",
    "{bdg-link-info}`DSFP Homepage <https://dsfp.norman-data.eu/>`\n",
    "{bdg-link-info}`Notebook Repository <hhttps://github.com/NERC-CEH/DSFP-PyExplorer>`\n",
    "{bdg-warning-line}`Ongoing Development`\n",
    "![alt text](../images/ukceh-logo-badge.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primary Contact: [Dr. Ezra Kitson](https://www.ceh.ac.uk/staff/ezra-kitson) | Notebook Top-to-Bottom Runtime ~ 2 Minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{card} Challenge:\n",
    "Environmental mass-spectrometry often returns data on many more compounds than just those compounds that were the target of the analysis. Non-target analysis is the analysis of the non-target compounds in an analytical spectrum. The first step in non-target analysis for mass spectrometry is non-target screening, which involves starting with a peak in the spectrum at a mass of interest and using all the available information (e.g. retention time, isotope peaks, fragment peaks) to go from a mass to an unequivocal molecular formula, and eventually a confirmed structure [1]. \n",
    "\n",
    "In recent years there has been work to standardise and refine this workflow [2], [3] resulting in the creation of the NORMAN Digital Sample Freezing Platform (DSFP), a web portal where users can upload raw mass spectrometry data from a particular project (known as a collection) and have the data automatically screened for non-target analysis peaks [3]. This non-target screening data is then available publicly for download.\n",
    "\n",
    "The second step of non-target analysis is to perform statistical analysis of the non-target screening data. There are currently a lack of tools to do this on the data available on the NORMAN-DSFP. The purpose of this work was to develop a Python library to make it easier to download, process and analyse the wealth of non-target analysis data available on the NORMAN-DSFP. \n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{card} Approach:\n",
    "We developed a Python library, DSFP-PyExplorer  to enable statistical analysis of non-target analysis data present on the NORMAN Digital Sample Freezing Platform  (DSFP). The library can be run from the command line and the scripts are parametrised by a single .yaml file. The library uses the REST API of the DSFP to download collections and then uses the Pandas and Numpy Python libraries to process the data into a metadata table and an ordination matrix, ready for statistical analysis \n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Running the Notebook:\n",
    ":class: tip, dropdown\n",
    "To run the notebook it is advised to first clone the repository housing the notebook ('*git clone https://github.com/NERC-CEH/DSFP-PyExplorer*'). This will create a folder in the current directory called *DSFP-PyEplorer*, which holds all the relevant files including the notebook, environment file and relevant input data.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{contents}\n",
    ":local:\n",
    "```\n",
    "<!-- https://jupyterbook.org/en/stable/structure/configure.html -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to use the DSFP-PyExplorer Python library, to download, process and analyse non target analysis mass spectrometry data located on the NORMAN Digital Sample Freezing Platform (DSFP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Downloading data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Data Access:\n",
    ":class: note, dropdown\n",
    "Life-APEX data has been predownloaded from the NORMAN-DSFP and stored in the GitHub repository housing this notebook. As such, you do not need to run the commands in the 'downloading data' section of this notebook, however if you would like to process a different data collection, feel free to do so. Life-APEX data is published by the University of Athens under an [open license.](https://dsfp.norman-data.eu/dataset/27df0a3e-3578-4a30-b9e4-1505f9da010d)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} LIFE-APEX:\n",
    ":class: note, dropdown\n",
    "The Life Apex project [collection ID 333] was a pan-EU initiative to perform targeted and non-targeted screening of biota, specifically Apex predators and their prey. To generate this data mass spectrometry was performed with two instrument setups, LC-ESI Positive mode-maXis QTOF (Submission #10) and LC ESI Negative mode-maXis QTOF (Submission #11) on tissue samples from various predator and prey animals prepared with a 50:50 water:methanol solvent. Scans were made from 50 – 1000 m/z, with a resolving power of 30000 for positive mode and 500 for negative mode.Further information on the method is available [4]. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Warning: Computational Demands\n",
    ":class: warning, dropdown\n",
    "The code in this section will take over ten minutes to run (longer if you choose to download a new data collection.)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data on the NORMAN DSFP is organised into collections. A collection is a group of samples submitted together to the DSFP which may represent a single sampling campaign, or multiple sampling campaigns across a project. With DSFP-PyExplorer we download the molecular data and metadata separately using two different scripts: ```ordination_mat.py``` downloads the molecular data and constructs an ordination matrix, and ```metadata.py``` downloads the associated metadata. \n",
    "\n",
    "To specify which collection to download we edit the ```config.yaml``` file in the root folder of the repository. Let's have a look an example ```.yaml``` file that will download data from the LIFE APEX collection."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#ordination_mat\n",
    "COLLECTION_ID: 333\n",
    "DOWNLOAD: True\n",
    "DOWNLOAD_DIR: \"../downloads/\"\n",
    "ORDPATH: \"../ordination/\"\n",
    "METHOD: \"max\"\n",
    "THREADS: 10\n",
    "\n",
    "#metadata\n",
    "METAPATH: \"metadata/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few aspects of this ```yaml``` file to unpack. Starting from top:\n",
    "* ```COLLECTION_ID``` instructs the script which data collection from the DSFP to process. \n",
    "* ```DOWNLOAD: True``` instructs the script to download molecular data from the NORMAN DSFP and store it in ```DOWNLOAD_DIR```. \n",
    "\n",
    "Next are arguments for constructing the ordination matrix:\n",
    "*  ```ORDPATH``` specifies where the ordination matrix should be saved to. \n",
    "* ```METHOD``` specifies how to deal with compounds that match to multiple peaks in a sample. ```max``` takes the maximum peak intensity value, ```sum```, sums the peak intensity values.  \n",
    "* ```THREADS``` specifies how many parallel CPU threads to use when downloading/loading/ordinating molecular data. \n",
    "\n",
    "The command line arguments below will run both scripts with the specified parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 94 collections.\n",
      "There are 36 participating labs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "download: 100%|██████████| 94/94 [00:06<00:00, 14.13it/s]\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "python ../metadata.py notebook_config.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load: 100%|██████████| 56248/56248 [03:33<00:00, 410.66it/s] \n",
      "load: 100%|██████████| 56248/56248 [03:39<00:00, 256.19it/s]\n",
      "\n",
      "ordinate:   0%|          | 1/56248 [04:40<4374:11:30, 279.96s/it]\u001b[A\n",
      "ordinate:   3%|▎         | 1806/56248 [04:40<1:38:31,  9.21it/s] \u001b[A\n",
      "ordinate:   7%|▋         | 3978/56248 [04:40<34:49, 25.01it/s]  \u001b[A\n",
      "ordinate:  11%|█         | 5913/56248 [04:40<18:37, 45.05it/s]\u001b[A\n",
      "ordinate:  19%|█▊        | 10538/56248 [04:40<06:42, 113.43it/s]\u001b[A\n",
      "ordinate:  34%|███▍      | 19325/56248 [04:40<02:03, 298.60it/s]\u001b[A\n",
      "ordinate:  43%|████▎     | 24052/56248 [04:40<01:13, 437.01it/s]\u001b[A\n",
      "ordinate:  51%|█████▏    | 28883/56248 [04:41<00:42, 637.93it/s]\u001b[A\n",
      "ordinate:  57%|█████▋    | 32162/56248 [04:41<00:29, 829.48it/s]\u001b[A\n",
      "ordinate:  73%|███████▎  | 41155/56248 [04:41<00:09, 1586.38it/s]\u001b[A\n",
      "ordinate:  82%|████████▏ | 46041/56248 [04:41<00:04, 2130.57it/s]\u001b[A\n",
      "ordinate:  89%|████████▉ | 50219/56248 [04:55<00:07, 856.28it/s] \u001b[A\n",
      "ordinate:  89%|████████▉ | 50250/56248 [04:55<00:07, 849.97it/s]\u001b[A\n",
      "ordinate:  94%|█████████▍| 53123/56248 [05:07<00:03, 849.97it/s]\u001b[A\n",
      "ordinate:  94%|█████████▍| 53124/56248 [05:07<00:06, 500.25it/s]\u001b[A\n",
      "ordinate:  94%|█████████▍| 53154/56248 [05:07<00:06, 497.23it/s]\u001b[A\n",
      "ordinate:  98%|█████████▊| 55182/56248 [05:21<00:03, 306.89it/s]\u001b[A\n",
      "ordinate:  98%|█████████▊| 55198/56248 [05:21<00:03, 305.41it/s]\u001b[A\n",
      "ordinate: 100%|██████████| 56248/56248 [06:09<00:00, 152.08it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python ../ordination_mat.py notebook_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the scripts are two ```.csv``` files that we can open using the ```pandas``` library in Python. We'll start by opening the ordination matrix. This consists of over 400 biological samples as rows, and over 50,000 molecules as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(413, 56248)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NS00037893</th>\n",
       "      <th>NS00058165</th>\n",
       "      <th>NS00043788</th>\n",
       "      <th>NS00043046</th>\n",
       "      <th>NS00047083</th>\n",
       "      <th>NS00017860</th>\n",
       "      <th>NS00060002</th>\n",
       "      <th>NS00083992</th>\n",
       "      <th>NS00029295</th>\n",
       "      <th>NS00073764</th>\n",
       "      <th>...</th>\n",
       "      <th>NS00096343</th>\n",
       "      <th>NS00014879</th>\n",
       "      <th>NS00068277</th>\n",
       "      <th>NS00032640</th>\n",
       "      <th>NS00100956</th>\n",
       "      <th>NS00024942</th>\n",
       "      <th>NS00024645</th>\n",
       "      <th>NS00066989</th>\n",
       "      <th>NS00059513</th>\n",
       "      <th>NS00080368</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19082</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>51058.1526</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54269.933000</td>\n",
       "      <td>8552.862857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21537</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4752.300889</td>\n",
       "      <td>23547.166500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19919</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>15188.6000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19129</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>172948.164843</td>\n",
       "      <td>251080.432380</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19871</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3433.188727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10713.063529</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56248 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       NS00037893  NS00058165  NS00043788  NS00043046  NS00047083  \\\n",
       "19082         NaN         NaN         NaN         NaN         NaN   \n",
       "21537         NaN         NaN         NaN         NaN         NaN   \n",
       "19919         NaN         NaN         NaN         NaN         NaN   \n",
       "19129         NaN         NaN         NaN         NaN         NaN   \n",
       "19871         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "        NS00017860  NS00060002  NS00083992  NS00029295  NS00073764  ...  \\\n",
       "19082          NaN         NaN         NaN         NaN         NaN  ...   \n",
       "21537          NaN         NaN         NaN         NaN         NaN  ...   \n",
       "19919          NaN         NaN         NaN         NaN         NaN  ...   \n",
       "19129          NaN         NaN         NaN         NaN         NaN  ...   \n",
       "19871  3433.188727         NaN         NaN         NaN         NaN  ...   \n",
       "\n",
       "       NS00096343     NS00014879     NS00068277    NS00032640  NS00100956  \\\n",
       "19082  51058.1526            NaN   54269.933000   8552.862857         NaN   \n",
       "21537         NaN            NaN    4752.300889  23547.166500         NaN   \n",
       "19919  15188.6000            NaN            NaN           NaN         NaN   \n",
       "19129         NaN  172948.164843  251080.432380           NaN         NaN   \n",
       "19871         NaN            NaN            NaN  10713.063529         NaN   \n",
       "\n",
       "       NS00024942  NS00024645  NS00066989  NS00059513  NS00080368  \n",
       "19082         NaN         NaN         NaN         NaN         NaN  \n",
       "21537         NaN         NaN         NaN         NaN         NaN  \n",
       "19919         NaN         NaN         NaN         NaN         NaN  \n",
       "19129         NaN         NaN         NaN         NaN         NaN  \n",
       "19871         NaN         NaN         NaN         NaN         NaN  \n",
       "\n",
       "[5 rows x 56248 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#read the ordination data (this will take a few seconds)\n",
    "ordinationData = pd.read_csv(\"../ordination/333_ordination.csv\", index_col=0)\n",
    "#print the matrix shape (rows x columns)\n",
    "print(ordinationData.shape)\n",
    "#print the first five rows \n",
    "ordinationData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metadata table helps us decipher what samples the row indices in the ordination matrix refer to. The 'ID' column of the metadata table maps to the index column of the ordination matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(436, 27)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Collection</th>\n",
       "      <th>Sample type</th>\n",
       "      <th>Short name for contribution</th>\n",
       "      <th>Sampling date</th>\n",
       "      <th>Analysis date</th>\n",
       "      <th>Instrument setup used</th>\n",
       "      <th>Monitored City</th>\n",
       "      <th>Monitored country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>...</th>\n",
       "      <th>Number of orgnanisms</th>\n",
       "      <th>Origin of biota</th>\n",
       "      <th>Proxy pressures</th>\n",
       "      <th>River basin name</th>\n",
       "      <th>Size (mm)</th>\n",
       "      <th>Species group</th>\n",
       "      <th>Tissue</th>\n",
       "      <th>Water content(%)</th>\n",
       "      <th>Weight (kg)</th>\n",
       "      <th>field_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>LIFE APEX</td>\n",
       "      <td>Real Sample</td>\n",
       "      <td>LIFE APEX 01 Bream muscle from Danube Jochenstein</td>\n",
       "      <td>Thu, 01/01/2015 - 00:00</td>\n",
       "      <td>Wed, 01/01/2020 - 00:00</td>\n",
       "      <td>LC-Bruker maXis QTOF: Submission #10</td>\n",
       "      <td>Jochenstein</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.83</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>River</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Danube river basin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Muscle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>LIFE APEX</td>\n",
       "      <td>Real Sample</td>\n",
       "      <td>LIFE APEX 02 Bream muscle from Elbe</td>\n",
       "      <td>Thu, 01/01/2015 - 00:00</td>\n",
       "      <td>Wed, 01/01/2020 - 00:00</td>\n",
       "      <td>LC-Bruker maXis QTOF: Submission #10</td>\n",
       "      <td>Wittenberge</td>\n",
       "      <td>Germany</td>\n",
       "      <td>53.01</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>River</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Elbe river basin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Muscle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>LIFE APEX</td>\n",
       "      <td>Real Sample</td>\n",
       "      <td>LIFE APEX 03 Bream muscle from Rhine Bimmen</td>\n",
       "      <td>Thu, 01/01/2015 - 00:00</td>\n",
       "      <td>Wed, 01/01/2020 - 00:00</td>\n",
       "      <td>LC-Bruker maXis QTOF: Submission #10</td>\n",
       "      <td>Emmerich</td>\n",
       "      <td>Germany</td>\n",
       "      <td>51.85</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>River</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rhine river basin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Muscle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>LIFE APEX</td>\n",
       "      <td>Real Sample</td>\n",
       "      <td>LIFE APEX 04 Bream muscle from Belauer See</td>\n",
       "      <td>Thu, 01/01/2015 - 00:00</td>\n",
       "      <td>Wed, 01/01/2020 - 00:00</td>\n",
       "      <td>LC-Bruker maXis QTOF: Submission #10</td>\n",
       "      <td>Neumunster</td>\n",
       "      <td>Germany</td>\n",
       "      <td>54.07</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>River</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Muscle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>LIFE APEX</td>\n",
       "      <td>Real Sample</td>\n",
       "      <td>LIFE APEX 05 Bream muscle from Saar Gudingen</td>\n",
       "      <td>Thu, 01/01/2015 - 00:00</td>\n",
       "      <td>Thu, 01/01/2015 - 00:00</td>\n",
       "      <td>LC-Bruker maXis QTOF: Submission #10</td>\n",
       "      <td>Saarbrucken</td>\n",
       "      <td>Germany</td>\n",
       "      <td>49.25</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>River</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rhine river basin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Muscle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID Collection  Sample type  \\\n",
       "0  37  LIFE APEX  Real Sample   \n",
       "1  38  LIFE APEX  Real Sample   \n",
       "2  39  LIFE APEX  Real Sample   \n",
       "3  40  LIFE APEX  Real Sample   \n",
       "4  41  LIFE APEX  Real Sample   \n",
       "\n",
       "                         Short name for contribution            Sampling date  \\\n",
       "0  LIFE APEX 01 Bream muscle from Danube Jochenstein  Thu, 01/01/2015 - 00:00   \n",
       "1                LIFE APEX 02 Bream muscle from Elbe  Thu, 01/01/2015 - 00:00   \n",
       "2        LIFE APEX 03 Bream muscle from Rhine Bimmen  Thu, 01/01/2015 - 00:00   \n",
       "3         LIFE APEX 04 Bream muscle from Belauer See  Thu, 01/01/2015 - 00:00   \n",
       "4       LIFE APEX 05 Bream muscle from Saar Gudingen  Thu, 01/01/2015 - 00:00   \n",
       "\n",
       "             Analysis date                 Instrument setup used  \\\n",
       "0  Wed, 01/01/2020 - 00:00  LC-Bruker maXis QTOF: Submission #10   \n",
       "1  Wed, 01/01/2020 - 00:00  LC-Bruker maXis QTOF: Submission #10   \n",
       "2  Wed, 01/01/2020 - 00:00  LC-Bruker maXis QTOF: Submission #10   \n",
       "3  Wed, 01/01/2020 - 00:00  LC-Bruker maXis QTOF: Submission #10   \n",
       "4  Thu, 01/01/2015 - 00:00  LC-Bruker maXis QTOF: Submission #10   \n",
       "\n",
       "  Monitored City Monitored country  Latitude  ...  Number of orgnanisms  \\\n",
       "0    Jochenstein               NaN     51.83  ...                   NaN   \n",
       "1    Wittenberge           Germany     53.01  ...                   NaN   \n",
       "2       Emmerich           Germany     51.85  ...                   NaN   \n",
       "3     Neumunster           Germany     54.07  ...                   NaN   \n",
       "4    Saarbrucken           Germany     49.25  ...                   NaN   \n",
       "\n",
       "  Origin of biota Proxy pressures    River basin name  Size (mm)  \\\n",
       "0           River             NaN  Danube river basin        NaN   \n",
       "1           River             NaN    Elbe river basin        NaN   \n",
       "2           River             NaN   Rhine river basin        NaN   \n",
       "3           River             NaN                 NaN        NaN   \n",
       "4           River             NaN   Rhine river basin        NaN   \n",
       "\n",
       "   Species group  Tissue  Water content(%) Weight (kg)  field_country  \n",
       "0            NaN  Muscle               NaN         NaN        Germany  \n",
       "1            NaN  Muscle               NaN         NaN        Germany  \n",
       "2            NaN  Muscle               NaN         NaN        Germany  \n",
       "3            NaN  Muscle               NaN         NaN        Germany  \n",
       "4            NaN  Muscle               NaN         NaN        Germany  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the metadata table\n",
    "metaData = pd.read_csv(\"../metadata/333_metadata.csv\", index_col=0)\n",
    "#print the matrix shape (rows x columns)\n",
    "print(metaData.shape)\n",
    "#print the first five rows \n",
    "metaData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "source": [
    "# Principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start to perform statistical analysis on the non target analysis data. Principal component analysis (PCA) is an unsupervised dimensionality reduction technique which is done by performing eigen decomposition of the covariance matrix of the ordination matrix, and then visualising the principal eigenvectors of this transformation. In simple language this allows us to visualise how similar samples are to one another along the main drivers of variation across compounds. DSFP-PyExplorer saves the sample scores (i.e. the PC1,PC2,…,PCN values for each sample), the PCA loadings (i.e. how each compound is transformed to generate the principal component aces) and PCA biplots and loading plots which can be styled using the metadata table. The library offers advanced options for peak-area normalisation and missing-data imputation, which reflect the latest statistical research. \n",
    "\n",
    "To perform PCA we first need to parameterise the ```.yaml``` file."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#data processing \n",
    "SUBSET: False\n",
    "\n",
    "#plotting\n",
    "PLOT: True\n",
    "HUE: \"Instrument setup used\"\n",
    "STYLE: \"Species group\"\n",
    "LEGEND_BBOX_X: 1.4\n",
    "LEGEND_BBOX_Y: -0.2\n",
    "\n",
    "#pca\n",
    "NORM_ORDER: \"rowcol\"\n",
    "ROW_METHOD: \"sum\"\n",
    "ROW_TRANSFORM: \"none\"\n",
    "COL_METHOD: \"center\"\n",
    "COL_TRANSFORM: \"power3\"\n",
    "COMPONENTS: 2\n",
    "LOADINGS: 20\n",
    "PCAPATH: \"pca/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Let's unpick the above, starting from the top: \n",
    "\n",
    "* ```SUBSET: \"Instrument type\"``` determines wheteher to subset the data before performing PCA. Set to ```none``` to do PCA on all the data. Subsetting means the data is divided based on a categorical variable in the metadata table, and PCA is performed independently on each division of the dataset. \n",
    "\n",
    "Next we have parameters thaat determine how the PCA biplots and loading plots should be made:\n",
    "\n",
    "* ```PLOT: True``` determines whether ```.svg.``` plot figures should be saved to ```PCAPATH```. \n",
    "* ```HUE: 'Instrument setup used'``` - determines the column of the metadata file to colour the scatter points by in PCA biplots. \n",
    "* ```STYLE: 'Species group'``` - determines the column of the metadata file to style the the scatter points by in PCA biplots. \n",
    "* ```LEGEND_BBOX_X``` and ```LEGEND_BBOX_Y``` determines where in the plot the legend should be placed. \n",
    "\n",
    "Then there are parameters about how to normalise the data before performing PCA. Normalisation can take place row-wise (to account for concentration differences between samples) and column-wise (to account for heteroskedasticity of compounds). A good primer on peak intensity normalisation can be read [here](https://link.springer.com/article/10.1186/1471-2164-7-142). Peak intensity normalisation is performed using the [PyKrev Python library](https://github.com/Kzra/pykrev). See the documentation for PyKrev.normalise_intensity() for more information on the parameters below.\n",
    "\n",
    "* ```NORM_ORDER: \"rowcol\"``` specifies what order to normalise the data in. One of ```row```, ```col```, ```rowcol```, ```colrow``` or ```none```.\n",
    "* ```ROW_METHOD: \"sum\"``` specifies how to row-normalise the ordination data before doing PCA. \n",
    "* ```ROW_TRANSFORM: \"power3\"``` specifies to row-transform the ordination data before doing PCA. \n",
    "* ```COL_METHOD: \"sum\"``` specifies how to column-normalise the ordination data before doing PCA. \n",
    "* ```COL_TRANSFORM: \"power3\"```specifies how to column-transform the ordination data before doing PCA.\n",
    "\n",
    "Finally there are parameters that determine the number of principal components, and loadings associated with those components to keep.\n",
    "\n",
    "* ```COMPONENTS: 4``` - Number of principal components to keep, PCA biplots will be made for all combinations of these loadings.\n",
    "* ```LOADINGS: 10``` - Number of loadings (ranked by absolute coefficient) to plot.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
